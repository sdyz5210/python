{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn的快速使用\n",
    "\n",
    "本文章原文地址：https://www.cnblogs.com/lianyingteng/p/7811126.html\n",
    "\n",
    "传统的机器学习任务从开始到建模的一般流程是：获取数据 -> 数据预处理 -> 训练建模 -> 模型评估 -> 预测，分类。本文我们将依据传统机器学习的流程，看看在每一步流程中都有哪些常用的函数以及它们的用法是怎么样的。\n",
    "\n",
    "## 获取数据\n",
    "\n",
    "### 导入数据\n",
    "\n",
    "sklearn中包含了大量的优质的数据集，在你学习机器学习的过程中，你可以通过使用这些数据集实现出不同的模型，从而提高你的动手实践能力，同时这个过程也可以加深你对理论知识的理解和把握。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris() # 导入数据集\n",
    "X = iris.data # 获得其特征向量\n",
    "y = iris.target # 获得样本label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=6, n_features=5, n_informative=2, \n",
    "    n_redundant=2, n_classes=2, n_clusters_per_class=2, scale=1.0, \n",
    "    random_state=20)\n",
    "\n",
    "# n_samples：指定样本数\n",
    "# n_features：指定特征数\n",
    "# n_classes：指定几分类\n",
    "# random_state：随机种子，使得随机状可重\n",
    "\n",
    "for x_,y_ in zip(X,y):\n",
    "    print y_,\":\",x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "\n",
    "数据预处理阶段是机器学习中不可缺少的一环，它会使得数据更加有效的被模型或者评估器识别。下面我们来看一下sklearn中有哪些平时我们常用的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据归一化处理\n",
    "\n",
    "为了使得训练数据的标准化规则与测试数据的标准化规则同步，preprocessing中提供了很多Scaler\n",
    "\n",
    "```\n",
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "#1. 基于mean和std的标准化\n",
    "scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "scaler.transform(train_data)\n",
    "scaler.transform(test_data)\n",
    "\n",
    "#2. 将每个特征值归一化到一个固定范围\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(train_data)\n",
    "scaler.transform(train_data)\n",
    "scaler.transform(test_data)\n",
    "#feature_range: 定义归一化范围，注用（）括起来\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化（normalize）\n",
    "\n",
    "当你想要计算两个样本的相似度时必不可少的一个操作，就是正则化。其思想是：首先求出样本的p-范数，然后该样本的所有元素都要除以该范数，这样最终使得每个样本的范数都为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[ 1., -1.,  2.],[ 2.,  0.,  0.],[ 0.,  1., -1.]]\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "X_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot编码\n",
    "\n",
    "one-hot编码是一种对离散特征值的编码方式，在LR模型中常用到，用于给线性模型增加非线性能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]\n",
    "encoder = preprocessing.OneHotEncoder().fit(data)\n",
    "encoder.transform(data).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集拆分\n",
    "\n",
    "在得到训练数据集时，通常我们经常会把训练数据集进一步拆分成训练集和验证集，这样有助于我们模型参数的选取。\n",
    "\n",
    "* 作用：将数据集划分为 训练集和测试集\n",
    "* 格式：train_test_split(*arrays, **options)\n",
    "\n",
    "    from sklearn.mode_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "```\n",
    "\n",
    "* 参数:\n",
    "\n",
    "arrays：样本数组，包含特征向量和标签\n",
    "\n",
    "test_size：\n",
    "　　float-获得多大比重的测试样本 （默认：0.25）\n",
    "　　int - 获得多少个测试样本\n",
    "\n",
    "train_size: 同test_size\n",
    "\n",
    "random_state:\n",
    "　　int - 随机种子（种子固定，实验可复现）\n",
    "　　\n",
    "shuffle - 是否在分割之前对数据进行洗牌（默认True）\n",
    "\n",
    "* 返回:\n",
    "\n",
    "分割后的列表，长度=2*len(arrays), \n",
    "　　(train-test split)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型\n",
    "\n",
    "* 拟合模型\n",
    "\n",
    "    model.fit(X_train, y_train)  \n",
    "\n",
    "* 模型预测\n",
    "\n",
    "    model.predict(X_test)  \n",
    "\n",
    "* 获得这个模型的参数\n",
    "    \n",
    "    model.get_params()  \n",
    "\n",
    "* 为模型进行打分\n",
    "    \n",
    "    model.score(data_X, data_y) # 线性回归：R square； 分类问题： acc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "定义线性回归模型\n",
    "\n",
    "model = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "参数\n",
    "\n",
    "---\n",
    "    fit_intercept：是否计算截距。False-模型没有截距\n",
    "    normalize： 当fit_intercept设置为False时，该参数将被忽略。 如果为真，则回归前的回归系数X将通过减去平均值并除以l2-范数而归一化。\n",
    "     n_jobs：指定线程数\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归LR\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "定义逻辑回归模型\n",
    "\n",
    "model = LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’liblinear’, max_iter=100, multi_class=’ovr’, verbose=0, warm_start=False, n_jobs=1)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "参数\n",
    "\n",
    "---\n",
    "    penalty：使用指定正则化项（默认：l2）\n",
    "    dual: n_samples > n_features取False（默认）\n",
    "    C：正则化强度的反，值越小正则化强度越大\n",
    "    n_jobs: 指定线程数\n",
    "    random_state：随机数生成器\n",
    "    fit_intercept: 是否需要常量\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯算法NB\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "model = naive_bayes.GaussianNB() # 高斯贝叶斯\n",
    "\n",
    "model = naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "model = naive_bayes.BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "文本分类问题常用MultinomialNB\n",
    "\n",
    "参数\n",
    "\n",
    "---\n",
    "    alpha：平滑参数\n",
    "    fit_prior：是否要学习类的先验概率；false-使用统一的先验概率\n",
    "    class_prior: 是否指定类的先验概率；若指定则不能根据参数调整\n",
    "    binarize: 二值化的阈值，若为None，则假设输入由二进制向量组成\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树DT\n",
    "\n",
    "from sklearn import tree \n",
    "\n",
    "model = tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None, \n",
    "    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "    max_features=None, random_state=None, max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "     class_weight=None, presort=False)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "参数\n",
    "\n",
    "---\n",
    "    criterion ：特征选择准则gini/entropy\n",
    "    max_depth：树的最大深度，None-尽量下分\n",
    "    min_samples_split：分裂内部节点，所需要的最小样本树\n",
    "    min_samples_leaf：叶子节点所需要的最小样本数\n",
    "    max_features: 寻找最优分割点时的最大特征数\n",
    "    max_leaf_nodes：优先增长到最大叶子节点数\n",
    "    min_impurity_decrease：如果这种分离导致杂质的减少大于或等于这个值，则节点将被拆分。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量机SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(C=1.0, kernel=’rbf’, gamma=’auto’)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "参数\n",
    "\n",
    "---\n",
    "    C：误差项的惩罚参数C\n",
    "    gamma: 核相关系数。浮点数，If gamma is ‘auto’ then 1/n_features will be used instead.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k近邻算法KNN\n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "#定义kNN分类模型\n",
    "\n",
    "model = neighbors.KNeighborsClassifier(n_neighbors=5, n_jobs=1) # 分类\n",
    "\n",
    "model = neighbors.KNeighborsRegressor(n_neighbors=5, n_jobs=1) # 回归\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "参数\n",
    "\n",
    "---\n",
    "    n_neighbors： 使用邻居的数目\n",
    "    n_jobs：并行任务数\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知机（神经网络）\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "定义多层感知机分类算法\n",
    "\n",
    "model = MLPClassifier(activation='relu', solver='adam', alpha=0.0001)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "参数\n",
    "\n",
    "---\n",
    "    hidden_layer_sizes: 元祖\n",
    "    activation：激活函数\n",
    "    solver ：优化算法{‘lbfgs’, ‘sgd’, ‘adam’}\n",
    "    alpha：L2惩罚(正则化项)参数。\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评估与选择篇\n",
    "\n",
    "## 交叉验证\n",
    "\n",
    "from sklearn.model_selection import cross_val_score cross_val_score(model, X, y=None, scoring=None, cv=None, n_jobs=1)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "参数\n",
    "\n",
    "---\n",
    "    model：拟合数据的模型\n",
    "    cv ： k-fold\n",
    "    scoring: 打分参数-‘accuracy’、‘f1’、‘precision’、‘recall’ 、‘roc_auc’、'neg_log_loss'等等\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检验曲线\n",
    "\n",
    "使用检验曲线，我们可以更加方便的改变模型参数，获取模型表现。\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "train_score, test_score = validation_curve(model, X, y, param_name, param_range, cv=None, scoring=None, n_jobs=1)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "参数\n",
    "\n",
    "---\n",
    "    model:用于fit和predict的对象\n",
    "    X, y: 训练集的特征和标签\n",
    "    param_name：将被改变的参数的名字\n",
    "    param_range： 参数的改变范围\n",
    "    cv：k-fold\n",
    "   \n",
    "返回值\n",
    "\n",
    "---\n",
    "   train_score: 训练集得分（array）\n",
    "    test_score: 验证集得分（array）\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型\n",
    "\n",
    "## 保存为pickle文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 保存模型\n",
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# 读取模型\n",
    "with open('model.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn自带方法joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(model, 'model.pickle')\n",
    "\n",
    "#载入模型\n",
    "model = joblib.load('model.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
